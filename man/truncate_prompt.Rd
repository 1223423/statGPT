% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/truncate_prompt.R
\name{truncate_prompt}
\alias{truncate_prompt}
\title{Truncate the code context}
\usage{
truncate_prompt(prompt, CONTEXT_LIMIT, MODEL)
}
\arguments{
\item{prompt}{(string) The complete prompt, including user query and context}

\item{CONTEXT_LIMIT}{(integer) The token limit (depends on model being used)}

\item{MODEL}{(string) The Open AI model being used}
}
\value{
(string) Truncated prompt that will fit inside the token limit
}
\description{
This function truncates the code context as accurately as possible in order to
maintain as large a context as possible. This is done through linked Python
code using OpenAI's tiktoken package. statGPT attempts to set up it's own
python environment with the correct version and install the requirements if
they aren't already.
}
